{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hồi quy tuyến tính\n",
    "*Hoàn thành toàn bộ phần bài tập trong notebook này, bao gồm toàn bộ kết quả đầu ra và code hỗ trợ.*\n",
    "\n",
    "***\"Không có một sự kiện nào trên đời là ngẫu nhiên, những thứ đang cho là ngẫu nhiên chỉ là những sự kiện ta chưa tìm ra được mô hình để biểu diễn quy luật của chúng\".***\n",
    "\n",
    "Xây dựng mô hình **Hồi quy tuyến tính** bao gồm hai phần:\n",
    "- Trong quá trình huấn luyện, bộ phân lớp lấy dữ liệu huấn luyện và và học các tham số mô hình.\n",
    "- Trong quá trình kiếm tra, mô hình phân lớp từng đối tượng bằng cách nhân giá trị của mẫu với các tham số mô hình để tìm ra giá trị của nhãn.\n",
    "- Giá trị của tham số được kiểm định chéo.\n",
    "Trong bài tập này, bạn sẽ cài đặt những bước trên và hiểu được qui trình Xây dựng một mô hình đơn giản với Học tham số, kiểm định chéo, và hiểu được cách viết code hiệu quả với vectorize.\n",
    "\n",
    "Bài toán dự đoán giá nhà Boston được sử dụng trong bài tập này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import một số thư viện cần thiết.\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sử dụng một mẹo nhỏ để vẽ hình trên cùng một dòng thay vì mở cửa sổ mới\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # đặt kích thước mặc định cho hình\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Một mẹo nhỏ để notebook tự load lại các module bên ngoài;\n",
    "# xem thêm tại http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tải dữ liệu Giá nhà Boston từ Scikit-learn.\n",
    "boston = datasets.load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, \\\n",
    "                                                    boston.target, test_size=0.2)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dữ liệu\n",
    "Hồi qui tuyến tính đơn giản là một cách tiếp cận để dự đoán phản ứng (giá trị đầu ra) khi dữ liệu có một đặc trưng duy nhất. Khi giả sử hai biến $x$ và $y$ liên hệ tuyến  tính thì mục tiêu của mô hình là cố tìm ra đường tuyến tính tốt nhất để dự đoán phản ứng ($y$). \n",
    "\n",
    "Đường đó được gọi là đường hồi quy.\n",
    "\n",
    "Công thức cho đường hồi quy được biểu diễn như sau:\n",
    "$$ \\hat{Y} = h(X) = XW$$\n",
    "Trong đó: \n",
    "\n",
    "- $X$ là ma trận có kích thước $N \\times D$ với $X_{ij}$ là giá trị của đặc trưng thứ $j$ của mẫu $i$.\n",
    "- $W$ là ma trận tham số có kích thước $D \\times 1$\n",
    "- $Y$ là giá trị phản ứng của $N$ mẫu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Biểu diễn một số ví dụ trong tập huấn luyện sử dụng một đặc trưng duy nhất.\n",
    "# LSTAT - % lower status of the population\n",
    "plt.scatter(X_train[:,12], y_train)\n",
    "plt.xlabel(\"Crime rate\")\n",
    "plt.ylabel(\"House's price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huấn luyện mô hình\n",
    "Tất cả code cho phần bài tập này được lưu trong tệp **models/linear_regression.py** và **models/linear_loss.py**.\n",
    "### Cập nhật tham số\n",
    "Quá trình huấn luyện mô hình thực chất là từ dữ liệu để học ra tham số mô hình phù hợp nhất với mô hình sinh dữ liệu. Trong mô hình hồi quy tuyến tính, ta cần học tham số $W$.\n",
    "\n",
    "Khi khởi tạo mô hình, ta giả sử tham số được khởi tạo ngẫu nhiên. Sử dụng tham số $W$ đó, ta ước lượng được giá trị $Y$:\n",
    "$$ \\hat{y} = h(X) = WX $$\n",
    "\n",
    "Tổng sai số, độ lệch của giá trị dự đoán so với giá trị thực tế gọi là hàm giá trị (Cost function):\n",
    "$$ J(w) = \\frac{1}{2N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2 = \\frac{1}{2N}\\sum_{i=1}^{N}\\sum_{j=1}^{D} (w_{j}x_{ij} - y_{ij})^2$$\n",
    "\n",
    "\n",
    "Chúng ta sử thuật toán **xuống đồi (Gradient descent)** để tối ưu tham số $W$. (Xem khóa [Machine Learning](https://www.coursera.org/learn/machine-learning/))\n",
    "\n",
    "Đột tụt dốc của tham số $W$ được cập nhật theo công thức:\n",
    "$$ dw_i = \\frac{\\partial}{\\partial w_i}J(w)$$\n",
    "\n",
    "Đầu tiên, mở file ```models/linear_loss.py``` và cài đặt hàm ```linear_loss_naive```, sử dụng vòng lặp để tính hàm giá trị (Cost function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models.linear_loss import linear_loss_naive\n",
    "import time\n",
    "\n",
    "# sinh ngẫu nhiên các trọng số (W) với các giá trị nhỏ\n",
    "W = np.random.randn(13, ) * 0.0001 \n",
    "\n",
    "loss, grad = linear_loss_naive(W, X_test, y_test, 0.00001)\n",
    "print('loss: %f' % (loss, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lúc này, các giá trị gradient được trả về đều bằng 0. Đạo hàm và tính gradient theo công thức được cho ở trên trong cùng hàm ```linear_loss_naive```. Bạn sẽ thấy một số thứ hữu ích trong phần cài đặt trước đó.\n",
    "\n",
    "Để đảm bảo là bạn đã cài đặt đúng, chúng ta sẽ sử dụng hàm ```grad_check_sparse``` (đã được cài đặt sẵn) để kiểm tra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bởi vì bạn đã cài đặt hàm gradient, tính toán gradient với code dưới đây và\n",
    "# kiểm tra với hàm grad_check_sparse(...) đã cho.\n",
    "\n",
    "# Tính toán loss và grad với W.\n",
    "loss, grad = linear_loss_naive(W, X_test, y_test, 0.0)\n",
    "\n",
    "# Tính toán gradient theo một số chiều ngẫu nhiên và so sánh chúng với kết quả\n",
    "# của bạn. Giá trị phải gần như chính xác theo tất cả các chiều.\n",
    "from models.gradient_check import grad_check_sparse\n",
    "f = lambda w: linear_loss_naive(w, X_test, y_test, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)\n",
    "\n",
    "# thực hiện kiểm tra khi có sử dụng regularization\n",
    "# đừng quên cài đặt gradient với regularization nhé.\n",
    "loss, grad = linear_loss_naive(W, X_test, y_test, 1e2)\n",
    "f = lambda w: linear_loss_naive(w, X_test, y_test, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)\n",
    "\n",
    "# Kết quả relative error trong khoảng 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kế tiếp, cài đặt linear_loss_vectorized; hiện tại chỉ tính toán hàm giá trị;\n",
    "# gradient sẽ cài đặt sau.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = linear_loss_naive(W, X_test, y_test, 0.00001)\n",
    "toc = time.time()\n",
    "print('Naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "# Vectorized\n",
    "from models.linear_loss import linear_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, _ = linear_loss_vectorized(W, X_test, y_test, 0.00001)\n",
    "toc = time.time()\n",
    "print('Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# Hàm giá trị khi vectorized nên có cùng giá trị với giá trị được tính bằng hàm\n",
    "# linear_loss_naive() nhưng tính toán nhanh hơn\n",
    "print('difference: %f' % (loss_naive - loss_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hoàn thiện phần cài đặt của linear_loss_vectorized, và tính toán gradient theo\n",
    "# cách vectorized.\n",
    "\n",
    "# Hai hàm tính loss và gradient nên cho kết quả giống nhau nhưng bản vectorized \n",
    "# tính toán nhanh hơn.\n",
    "tic = time.time()\n",
    "_, grad_naive = linear_loss_naive(W, X_test, y_test, 0.00001)\n",
    "toc = time.time()\n",
    "print('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = linear_loss_vectorized(W, X_test, y_test, 0.00001)\n",
    "toc = time.time()\n",
    "print('Vectorized loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "# So sánh gradient\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized)\n",
    "print('difference: {}'.format(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Huấn luyện với hàm cập nhật\n",
    "Sử dụng các hàm ```loss``` đã cài đặt ở trên để cài đặt hàm ```train``` trong tệp **linear_regression.py**.\n",
    "\n",
    "Tham số W được cập nhật từng thành phần theo công thức:\n",
    "$$ w_i =  w_i -\\alpha\\frac{\\partial}{\\partial w_i}J(w)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ở trong tệp linear_regression.py, cài đặt hàm LinearRegression.train() và chạy\n",
    "# hàm đó với code sau\n",
    "from models.linear_regression import LinearRegression\n",
    "clf = LinearRegression()\n",
    "tic = time.time()\n",
    "loss_hist = clf.train(X_train, y_train, learning_rate=1e-7, reg=5e4,\n",
    "                      num_iters=1500, verbose=True)\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Một chiến thuật debug hiệu quả được sử dụng đó là vẽ ra lịch sử mất mát (loss \n",
    "# history) như là một hàm với số lần lặp.\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt hàm LinearRegression.predict đánh giá hiệu năng mô hình trên cả tập\n",
    "# huấn luyện và tệp kiểm tra.\n",
    "y_train_pred = clf.predict(X_train)\n",
    "\n",
    "# TODO: Tính toán hệ số tương quan của đầu ra mô hình và kết quả thực sự\n",
    "train_coeff = None\n",
    "print('training coefficient: %f' % (train_coeff))\n",
    "y_test_pred = clf.predict(X_test)\n",
    "test_coeff = None\n",
    "print('validation coefficient: %f' % (test_coeff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sự chính quy hóa (Regularization)\n",
    "\n",
    "Regularization, một cách cơ bản, là thay đổi mô hình một chút để tránh overfitting trong khi vẫn giữ được tính tổng quát của nó (tính tổng quát là tính mô tả được nhiều dữ liệu, trong cả tập training và test).\n",
    "\n",
    "Có rất nhiều phương pháp được sử dụng để chính quy hóa một mô hình, trong đó, chúng ta sẽ tìm hiểu về phương pháp $L_2 \\, regularization$.\n",
    "\n",
    "#### Regularized loss function\n",
    "Kỹ thuật regularization phổ biến nhất là thêm vào hàm mất mát một số hạng nữa. Số hạng này thường dùng để đánh giá độ phức tạp của mô hình. Số hạng này càng lớn, thì mô hình càng phức tạp. Hàm mất mát mới này thường được gọi là `regularized loss function`, thường được định nghĩa như sau:\n",
    "$$ J_{reg}(\\theta) = J(\\theta) + \\lambda R(\\theta)$$\n",
    "\n",
    "Trong đó:\n",
    "- $\\theta$ là tham số mô hình (trong trường hợp này là $w$)\n",
    "- $\\lambda$ là hằng số regularization\n",
    "- $R(\\theta)$ là số hạng regularization\n",
    "\n",
    "Kỹ thuật `l2 regularization`sử dụng $R(\\theta) = R(w) = ||w||_2^2$.\n",
    "\n",
    "#### Cập nhật tham số\n",
    "Do tham số được cập nhật dựa trên đạo hàm của hàm mất mát, ta cần xây dựng lại công thức tính đạo hàm theo cách sau:\n",
    "$$ w_i =  w_i -\\alpha\\frac{\\partial}{\\partial w_i}J_{reg}(w)$$\n",
    "\n",
    "** Bài tập:** Dựa trên 2 công thức chuẩn hóa có ở trên, viết lại code cho bộ phân lớp `Linear Regression` và chạy tiếp code sau để tìm được mô hình tối ưu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sử dụng tập kiểm tra để điều chỉnh các siêu tham số (độ lớn của reg và tỉ\n",
    "# lệ học. Bạn nên thực nghiệm với nhiều khoảng giá trị của 2 siêu tham số này\n",
    "learning_rates = [1e-7, 5e-5]\n",
    "regularization_strengths = [5e4, 1e5]\n",
    "\n",
    "# kết quả là một từ điển ánh xạ từ tuple có dạng (reg, lr) sang tuple có dạng\n",
    "# (train_coeff, test_coeff). Độ chính xác chỉ đơn giản là tỉ lệ mẫu dự đoán chính\n",
    "# xác trên toàn tập dữ liệu.\n",
    "results = {}\n",
    "best_coeff = -1    # Hệ số tương quan tốt nhất mà chúng ta sẽ đạt được.\n",
    "best_linear = None # Mô hình LinearRegression có hiệu năng tốt nhất.\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Viết code chọn các siêu tham số tốt nhất bằng cách điều chỉnh trên tập kiểm  #\n",
    "# tra. Với mỗi tổ hợp siêu tham số, huấn luyện một mô hình LinearRegression    #\n",
    "# trên tập huấn luyện, tính toán độ tương quan trên tập huấn luyện và tập kiểm #\n",
    "# tra, và lưu những con số này vào từ điển kết quả. Thêm vào đó, lưu hiệu năng #\n",
    "# tốt nhất trên tập kiểm tra vào best_val và mô hình LinearRegression tương    #\n",
    "# ứng vào best_svm.                                                            #  \n",
    "#                                                                              #\n",
    "# Gợi ý: Bạn nên sử dụng số vòng lặp (num_iters) nhỏ khi xây dựng code kiểm    #\n",
    "# tra để mô hình không mất quá nhiều thời gian để huấn luyện. Khi đã chắc chắn,#\n",
    "# bạn nên trả về kết quả với số vòng lặp lớn                                   #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              KẾT THÚC                                        #\n",
    "################################################################################\n",
    "    \n",
    "# In kết quả\n",
    "for lr, reg in sorted(results):\n",
    "    train_coeff, test_coeff = results[(lr, reg)]\n",
    "    print('lr %e reg %e train coefficient: %f val coefficient: %f' % (\n",
    "                lr, reg, train_coeff, test_coeff))\n",
    "    \n",
    "print('best validation coefficient achieved during cross-validation: %f' % best_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize kết quả kiểm thử chéo\n",
    "import math\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# plot training accuracy\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('Boston training accuracy')\n",
    "\n",
    "# vẽ hiệu năng trên tập kiểm tra\n",
    "colors = [results[x][1] for x in results] # kích thước mặc định của marker là 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('Boston test accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
